{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Ejercicio de $Q$-learning. _Rock, paper, scissors, lizard, Spock_<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto D铆az lvarez<br>Grupo: NeoTech<br>ltima actualizaci贸n: 2023-05-06</small></i></div>\n",
    "\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducci贸n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En inteligencia artificial, el **aprendizaje por refuerzo** es una t茅cnica de aprendizaje autom谩tico que permite que un agente aprenda a **tomar decisiones a trav茅s de la interacci贸n con un entorno**. En este contexto, el algoritmo $Q$-learning es una t茅cnica popular para el aprendizaje por refuerzo que permite que un agente aprenda a maximizar una recompensa a largo plazo al tomar decisiones 贸ptimas en un entorno incierto.\n",
    "\n",
    "En esta pr谩ctica, se explorar谩 el uso del algoritmo Q-learning para entrenar a un agente para que juegue al juego _rock, paper, scissors, lizard, spock_ a partir de informaci贸n extra铆da de un jugardor con ciertos sesgos, cosa que le sucede generalmente a los humanos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a desarrollar un agente para que aprenda a jugar al juego de manera relativamente competente jugando al juego indicado usando para ello $Q$-learning.\n",
    "\n",
    "Primero desarrollaremos el algoritmo b谩sico para aprender de unos datos extra铆dos de un jugador. Luego, se pedir谩 que se altere (si se desea) el agente para quu juegue contra los agentes deel resto de los estudiantes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports y configuraci贸n\n",
    "\n",
    "A continuaci贸n importaremos las librer铆as que se usar谩n a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import typing\n",
    "import urllib\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripci贸n del problema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este juego es una extensi贸n del cl谩sico juego 芦piedra, papel o tijera禄 e incluye dos elementos adicionales, 芦lagarto禄 (_lizard_ en ingl茅s) y 芦Spock禄 (extraterrestre de la raza Vulcana, conocido por su l贸gica y falta de emociones). El objetivo del juego es seleccionar una de las cinco opciones posibles y ganar a la opci贸n elegida por el oponente, siguiendo las siguientes reglas:\n",
    "\n",
    "- La piedra aplasta la tijera y aplasta al lagarto\n",
    "- La tijera corta el papel y decapita al lagarto\n",
    "- El papel envuelve la piedra y desautoriza a Spock\n",
    "- El lagarto envenena a Spock y come el papel\n",
    "- Spock rompe las tijeras y vaporiza la piedra\n",
    "\n",
    "Comenzamos definiendo las opciones posibles del juego, que se corresponder谩n con las acciones posibles que puede realizar un agente en el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(enum.Enum):\n",
    "    \"\"\"Cada una de las posibles figuras.\"\"\"\n",
    "    ROCK = ''\n",
    "    PAPER = 'Щ'\n",
    "    SCISSORS = '锔'\n",
    "    LIZARD = ''\n",
    "    SPOCK = ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los movimientos junto con sus recompensas (si no nos hemos equivocado) se representan en el siguiente diccionario. Las recompensas se establecen en 1, 0, -1 dependiendo de si la primera opci贸n gana a, empata con o pierde frente a la segunda, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVES_AND_REWARDS = {\n",
    "    (Action.ROCK, Action.ROCK): 0, (Action.ROCK, Action.PAPER): -1,\n",
    "    (Action.ROCK, Action.SCISSORS): 1, (Action.ROCK, Action.LIZARD): 1,\n",
    "    (Action.ROCK, Action.SPOCK): -1,\n",
    "    (Action.PAPER, Action.ROCK): 1, (Action.PAPER, Action.PAPER): 0,\n",
    "    (Action.PAPER, Action.SCISSORS): -1, (Action.PAPER, Action.LIZARD): -1,\n",
    "    (Action.PAPER, Action.SPOCK): 1,\n",
    "    (Action.SCISSORS, Action.ROCK): -1, (Action.SCISSORS, Action.PAPER): 1,\n",
    "    (Action.SCISSORS, Action.SCISSORS): 0, (Action.SCISSORS, Action.LIZARD): 1,\n",
    "    (Action.SCISSORS, Action.SPOCK): -1,\n",
    "    (Action.LIZARD, Action.ROCK): -1, (Action.LIZARD, Action.PAPER): 1,\n",
    "    (Action.LIZARD, Action.SCISSORS): -1, (Action.LIZARD, Action.LIZARD): 0,\n",
    "    (Action.LIZARD, Action.SPOCK): 1,\n",
    "    (Action.SPOCK, Action.ROCK): 1, (Action.SPOCK, Action.PAPER): -1,\n",
    "    (Action.SPOCK, Action.SCISSORS): 1, (Action.SPOCK, Action.LIZARD): -1,\n",
    "    (Action.SPOCK, Action.SPOCK): 0,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta informaci贸n, crearemos el juego para que el agente (humano o m谩quina) pueda jugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    RENDER_MODE_HUMAN = 'human'\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def play(self, p1_action, p2_action):\n",
    "        result = MOVES_AND_REWARDS[(p1_action, p2_action)]\n",
    "        if self.render_mode == 'human':\n",
    "            self.render(p1_action, p2_action, result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def render(p1_action, p2_action, result):\n",
    "        if result == 0:\n",
    "            print(f'{p1_action.value} tie!')\n",
    "        elif result == 1:\n",
    "            print(f'{p1_action.value} beats {p2_action.value}')\n",
    "        elif result == -1:\n",
    "            print(f'{p2_action.value} beats {p1_action.value}')\n",
    "        else:\n",
    "            raise ValueError(f'{p1_action}, {p2_action}, {result}')\n",
    "\n",
    "game = Game(render_mode='human')\n",
    "game.play(np.random.choice(list(Action)), np.random.choice(list(Action)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente que aprende mediante $Q$-learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una clase auxiliar denominada `Transition` que va a guardar la informaci贸n de una transici贸n, definida por el estado origen, el estado destino, la acci贸n por la que ocurri贸 dicha transici贸n y la recompensa de la misma. De esta manera tendremos en un mismo objeto toda la informaci贸n relacionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(typing.NamedTuple):\n",
    "    \"\"\"Representa la transici贸n de un estado al siguiente\"\"\"\n",
    "    prev_state: int              # Estado origen de la transici贸n\n",
    "    next_state: int              # Estado destino de la transici贸n\n",
    "    action: Action               # Acci贸n que provoc贸 esta transici贸n\n",
    "    reward: typing.SupportsFloat # Recompensa obtenida"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estado lo representaremos como un entero ya que, en este problema en concreto, s贸lo tenemos un estado. Hemos preferido mantener a煤n as铆 los atributos `prev_state` y `next_state` para que el c贸digo se corresponda con las f贸rmulas explicadas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1. Definici贸n del agente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos el agente que aprender谩 a jugar a partir de las jugadas de los dem谩s. Debe implementar los m茅todos indicados para que cumplan la firma y el comentario.\n",
    "\n",
    "Se permite escribir c贸digo s贸lo entre el espacio delimitado por los comentarios `# START` y `# END`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import requests\n",
    "\n",
    "\n",
    "# Hemos cambiado alg煤n nombre por comodidad\n",
    "class Agent:\n",
    "    def __init__(self, name: str, q_table: typing.Any=None):\n",
    "        \"\"\"Inicializa este objeto.\n",
    "        \n",
    "        :param name: El nombre del agente, para identificarle.\n",
    "        :param q_table: Una tabla q de valores. Es opcional.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        if q_table:\n",
    "            self.q_table = q_table\n",
    "        else:\n",
    "            # Inicializa la Q-table a 0\n",
    "            self.q_table = {b.value: 0.0 for b in Action}\n",
    "            self.updated_table = self.q_table.copy()\n",
    "            \n",
    "\n",
    "    def decide(self, state:int, epsilon: typing.SupportsFloat=0) -> Action:\n",
    "        \"\"\"Decide la acci贸n a ejecutar.\n",
    "        \n",
    "        :param state: El estado en el que nos encontramos.\n",
    "        :param epsilon: Un valor entre 0 y 1 que representa, seg煤n la estrategia\n",
    "            蔚-greedy, la probabilidad de que la acci贸n sea elegida de manera\n",
    "            aleatoria de entre todas las opciones posibles. Es opcional, y si\n",
    "            no se especifica su valor es 0 (sin probabilidades de que se elija\n",
    "            una acci贸n aleatoria).\n",
    "        :param returns: La acci贸n a ejecutar.\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            # Con probabilidad epsilon, elegir una acci贸n random\n",
    "            return np.random.choice(list(Action))\n",
    "        if state in self.q_table:\n",
    "            # Si el estado est谩 en la Q-table, elige el que tenga el valor m谩s alto\n",
    "            argmax = np.argmax(list(self.q_table[state].values()))\n",
    "            return list(Action)[argmax]\n",
    "        else:\n",
    "            # Si no lo est谩, elige uno random\n",
    "            self.q_table[state] = {action.value: 0.0 for action in Action}\n",
    "            return np.random.choice(list(Action))\n",
    "        \n",
    "\n",
    "    def update(self, t: Transition, alpha=0.1, gamma=0.95):\n",
    "        \"\"\"Actualiza el estado interno de acuerdo a la experiencia vivida.\n",
    "        \n",
    "        :param transition: Toda la informaci贸n correspondiente a la transici贸n\n",
    "            de la que queremos aprender.\n",
    "        :param : El factor de aprendizaje del cambio en el valor q. Por\n",
    "            defecto es 0.1\n",
    "        :param : La influencia de la recompensa a largo plazo en el valor q a\n",
    "            actualizar. Va de 0 (sin influencia) a 1 (misma influencia que el\n",
    "            valor actual). Por defecto es 0.95.\n",
    "        \"\"\"\n",
    "        if t.prev_state not in self.q_table:\n",
    "            # Si el prev estado no est谩, a帽adirlo\n",
    "            self.q_table[t.prev_state] = {action.value: 0.0 for action in Action}\n",
    "        current_q = self.q_table[t.prev_state][t.action.value]\n",
    "        next_q = max(list(self.q_table[t.next_state].values()))\n",
    "        td_target = t.reward + gamma * next_q\n",
    "        td_error = td_target - current_q\n",
    "        new_q = current_q + alpha * td_error\n",
    "        self.q_table[t.prev_state][t.action.value] = new_q\n",
    "        self.updated_table = self.q_table.copy()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Representaci贸n textual de la tabla Q del agente.\n",
    "        \n",
    "        :returns: Una cadena indicando la estructura interna de la tabla Q.\n",
    "        \"\"\"\n",
    "        table_str = \"Agent - Q Table:\\n\"\n",
    "        for state, actions in self.q_table.items():\n",
    "            table_str += f\"State: {state}\\n\"\n",
    "            if isinstance(actions, float):\n",
    "                table_str += f\"  {Action(state).name}: {actions}\\n\"\n",
    "            else:\n",
    "                for action, value in actions.items():\n",
    "                    table_str += f\"  {action}: {value}\\n\"\n",
    "        return table_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrenamiento usaremos el dataset localizado en <https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn>, el cual contiene una secuencia de opciones que ha tomado un jugador en una partida ficticia de este juego. Comenzamos cargando el fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el dataset descargado, ya podemos realizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " = 1\n",
    "筐 =  / len(player2_actions)\n",
    "\n",
    "game = Game()\n",
    "agent = Agent(name='Agent')\n",
    "\n",
    "state = 0  # El entorno (juego) no tiene estado, as铆 que siempre ser谩 el mismo\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state, )\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "\n",
    "    # Actualizamos el agente\n",
    "    agent.update(Transition(\n",
    "        prev_state=state,\n",
    "        next_state=state,\n",
    "        action=p1_action,\n",
    "        reward=reward\n",
    "    ))\n",
    "\n",
    "    # Actualizamos \n",
    "     -= 筐 if  > 0 else 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras el entrenamiento, podemos ver c贸mo est谩n de repartidos los valores de la tabla $Q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qu茅 tal se comporta el con un conjunto de datos que nunca ha visto del mismo contrincante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.tst'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])\n",
    "\n",
    "stats = collections.defaultdict(int)\n",
    "state = 0\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "    if reward == 1:\n",
    "        stats['wins'] += 1\n",
    "    elif reward == -1:\n",
    "        stats['loses'] += 1\n",
    "    else:\n",
    "        stats['ties'] += 1\n",
    "\n",
    "print(dict(stats))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo visto ha aprendido un poco del sesgo que ten铆a el contrincante. 隆Bien por nuestro agente! Ya est谩 un poco m谩s cerca de dominar el mundo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2. Nuevo agente para competici贸n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer las cosas un poco m谩s interesantes, vamos a hacer una peque帽a competici贸n. Se trata de desarrollar un agente como el que acabamos de hacer, pero donde los contrincantes ser谩n el resto de agentes de los grupos. Se permite cualquier implementaci贸n, siempre y cuando:\n",
    "\n",
    "- Herede de la clase `Agent` suministrada.\n",
    "- El m茅todo `__init__` no admita ning煤n par谩metro adicional.\n",
    "\n",
    "En la siguiente celda se ofrece el esqueleto del agente que competir谩. Como se puede ver:\n",
    "\n",
    "- Tiene un nombre\n",
    "- A la hora de decidir la acci贸n recibir谩 un estado, que siempre ser谩 0 y devolver谩 la opci贸n a realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Agent(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "        \n",
    "        :param name: El nombre del agente.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        # START\n",
    "        # END\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decide(self, state:int) -> Action:\n",
    "        \"\"\"Decide la acci贸n a llevar a cabo dado el estado actual.\n",
    "        \n",
    "        :param state: El estado en el que se encuentra el agente.\n",
    "        :returns: La acci贸n a llevar a cabo.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        # END\n",
    "    \n",
    "    def update(self, transition: Transition):\n",
    "        \"\"\"Actualiza (si es necesario) el estado interno del agente.\n",
    "        \n",
    "        :param transition: La informaci贸n de la transici贸n efectuada.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, un par de posibles implementaciones (que no usan nada de $q$-learning, y que no se recomiendan) son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Botnifacio(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Botnifacio')\n",
    "    \n",
    "    def decide(self, state:int) -> Action:\n",
    "        return np.random.choice(list(Action))\n",
    "\n",
    "\n",
    "class Gustabot(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Gustabot')\n",
    "        self.weights = np.random.random(5)\n",
    "        self.weights /= sum(self.weights)\n",
    "    \n",
    "    def decide(self, state:int) -> Action:\n",
    "        return np.random.choice(list(Action), p=self.weights)\n",
    "    \n",
    "    def update(self, transition: Transition):\n",
    "        self.weights = np.random.random(5)\n",
    "        self.weights /= sum(self.weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente se deber谩 desarrollar en la siguiente celda, la cual incluir谩 todo lo necesario para instanciarlo y que funcione. Se recomienda la competici贸n entre grupos antes de la entrega final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar aqu铆 al agente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La competici贸n ser谩 una liga entre los agentes de cada grupo a 100 partidas.\n",
    "\n",
    "Antes de dichas 100 partidas, se har谩 un previo de 10000 partidas con el fin de que cada uno de los agentes \"aprenda\" a competir contra el contrario. S贸lo en esta fase se invocar谩 el m茅todo `update`.\n",
    "\n",
    "La competici贸n se realizar谩 como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "FRIENDLY_SETS = 10000\n",
    "COMPETITION_SETS = 10\n",
    "\n",
    "competitors = [\n",
    "    Gustabot(),\n",
    "    Botnifacio(),\n",
    "]\n",
    "\n",
    "leaderboard = {c: 0 for c in sorted(competitors, key=lambda x: x.__str__())}\n",
    "game = Game()\n",
    "for p1, p2 in itertools.combinations(leaderboard.keys(), 2):\n",
    "    # Amistoso\n",
    "    s = 0\n",
    "    for i in range(FRIENDLY_SETS):\n",
    "        a1 = p1.decide(s)\n",
    "        a2 = p2.decide(s)\n",
    "        reward = game.play(a1, a2)\n",
    "        p1.update(Transition(prev_state=s, next_state=s, action=a1, reward=reward))\n",
    "        p2.update(Transition(prev_state=s, next_state=s, action=a2, reward=-reward))\n",
    "    \n",
    "    # Competici贸n\n",
    "    s = 0\n",
    "    r1 = r2 = 0\n",
    "    for i in range(COMPETITION_SETS):\n",
    "        a1 = p1.decide(s)\n",
    "        a2 = p2.decide(s)\n",
    "        reward = game.play(a1, a2)\n",
    "        r1 += reward\n",
    "        r2 -= reward\n",
    "\n",
    "    # Actualizaci贸n de marcadores globales\n",
    "    leaderboard[p1] += r1\n",
    "    leaderboard[p2] += r2\n",
    "    \n",
    "    print(f'{p1}: {r1}, {p2}: {r2}')\n",
    "    \n",
    "\n",
    "print('LEADERBOARD')\n",
    "for c, r in sorted(leaderboard.items(), key=lambda t: t[1], reverse=True):\n",
    "    print(f'{r:<10}\\t{c}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La calificaci贸n de cada grupo ir谩 en funci贸n de la puntuaci贸n final que haya conseguido."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumiendo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos desarrollado un agente para un juego diferente que aprender a tomar decisiones 贸ptimas en cada situaci贸n, a partir de la retroalimentaci贸n proporcionada por el entorno del juego. Hemos observado c贸mo el agente ha ido mejorando gradualmente su desempe帽o en el juego, a medida que ha acumulado experiencia y ha ajustado sus valores de Q.\n",
    "\n",
    "Adem谩s, hemos podido comprobar c贸mo el agente es capaz de adaptarse a diferentes estrategias de juego de su oponente, lo que demuestra su capacidad para generalizar y tomar decisiones en situaciones nuevas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
