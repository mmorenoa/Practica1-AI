{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Ejercicio de $Q$-learning. _Rock, paper, scissors, lizard, Spock_<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Grupo: NeoTech<br>Última actualización: 2023-05-06</small></i></div>\n",
    "\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En inteligencia artificial, el **aprendizaje por refuerzo** es una técnica de aprendizaje automático que permite que un agente aprenda a **tomar decisiones a través de la interacción con un entorno**. En este contexto, el algoritmo $Q$-learning es una técnica popular para el aprendizaje por refuerzo que permite que un agente aprenda a maximizar una recompensa a largo plazo al tomar decisiones óptimas en un entorno incierto.\n",
    "\n",
    "En esta práctica, se explorará el uso del algoritmo Q-learning para entrenar a un agente para que juegue al juego _rock, paper, scissors, lizard, spock_ a partir de información extraída de un jugardor con ciertos sesgos, cosa que le sucede generalmente a los humanos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a desarrollar un agente para que aprenda a jugar al juego de manera relativamente competente jugando al juego indicado usando para ello $Q$-learning.\n",
    "\n",
    "Primero desarrollaremos el algoritmo básico para aprender de unos datos extraídos de un jugador. Luego, se pedirá que se altere (si se desea) el agente para quu juegue contra los agentes deel resto de los estudiantes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports y configuración\n",
    "\n",
    "A continuación importaremos las librerías que se usarán a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import typing\n",
    "import urllib\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este juego es una extensión del clásico juego «piedra, papel o tijera» e incluye dos elementos adicionales, «lagarto» (_lizard_ en inglés) y «Spock» (extraterrestre de la raza Vulcana, conocido por su lógica y falta de emociones). El objetivo del juego es seleccionar una de las cinco opciones posibles y ganar a la opción elegida por el oponente, siguiendo las siguientes reglas:\n",
    "\n",
    "- La piedra aplasta la tijera y aplasta al lagarto\n",
    "- La tijera corta el papel y decapita al lagarto\n",
    "- El papel envuelve la piedra y desautoriza a Spock\n",
    "- El lagarto envenena a Spock y come el papel\n",
    "- Spock rompe las tijeras y vaporiza la piedra\n",
    "\n",
    "Comenzamos definiendo las opciones posibles del juego, que se corresponderán con las acciones posibles que puede realizar un agente en el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(enum.Enum):\n",
    "    \"\"\"Cada una de las posibles figuras.\"\"\"\n",
    "    ROCK = '🪨'\n",
    "    PAPER = '🧻'\n",
    "    SCISSORS = '✂️'\n",
    "    LIZARD = '🦎'\n",
    "    SPOCK = '🖖'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los movimientos junto con sus recompensas (si no nos hemos equivocado) se representan en el siguiente diccionario. Las recompensas se establecen en 1, 0, -1 dependiendo de si la primera opción gana a, empata con o pierde frente a la segunda, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVES_AND_REWARDS = {\n",
    "    (Action.ROCK, Action.ROCK): 0, (Action.ROCK, Action.PAPER): -1,\n",
    "    (Action.ROCK, Action.SCISSORS): 1, (Action.ROCK, Action.LIZARD): 1,\n",
    "    (Action.ROCK, Action.SPOCK): -1,\n",
    "    (Action.PAPER, Action.ROCK): 1, (Action.PAPER, Action.PAPER): 0,\n",
    "    (Action.PAPER, Action.SCISSORS): -1, (Action.PAPER, Action.LIZARD): -1,\n",
    "    (Action.PAPER, Action.SPOCK): 1,\n",
    "    (Action.SCISSORS, Action.ROCK): -1, (Action.SCISSORS, Action.PAPER): 1,\n",
    "    (Action.SCISSORS, Action.SCISSORS): 0, (Action.SCISSORS, Action.LIZARD): 1,\n",
    "    (Action.SCISSORS, Action.SPOCK): -1,\n",
    "    (Action.LIZARD, Action.ROCK): -1, (Action.LIZARD, Action.PAPER): 1,\n",
    "    (Action.LIZARD, Action.SCISSORS): -1, (Action.LIZARD, Action.LIZARD): 0,\n",
    "    (Action.LIZARD, Action.SPOCK): 1,\n",
    "    (Action.SPOCK, Action.ROCK): 1, (Action.SPOCK, Action.PAPER): -1,\n",
    "    (Action.SPOCK, Action.SCISSORS): 1, (Action.SPOCK, Action.LIZARD): -1,\n",
    "    (Action.SPOCK, Action.SPOCK): 0,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta información, crearemos el juego para que el agente (humano o máquina) pueda jugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    RENDER_MODE_HUMAN = 'human'\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def play(self, p1_action, p2_action):\n",
    "        result = MOVES_AND_REWARDS[(p1_action, p2_action)]\n",
    "        if self.render_mode == 'human':\n",
    "            self.render(p1_action, p2_action, result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def render(p1_action, p2_action, result):\n",
    "        if result == 0:\n",
    "            print(f'{p1_action.value} tie!')\n",
    "        elif result == 1:\n",
    "            print(f'{p1_action.value} beats {p2_action.value}')\n",
    "        elif result == -1:\n",
    "            print(f'{p2_action.value} beats {p1_action.value}')\n",
    "        else:\n",
    "            raise ValueError(f'{p1_action}, {p2_action}, {result}')\n",
    "\n",
    "game = Game(render_mode='human')\n",
    "game.play(np.random.choice(list(Action)), np.random.choice(list(Action)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente que aprende mediante $Q$-learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una clase auxiliar denominada `Transition` que va a guardar la información de una transición, definida por el estado origen, el estado destino, la acción por la que ocurrió dicha transición y la recompensa de la misma. De esta manera tendremos en un mismo objeto toda la información relacionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(typing.NamedTuple):\n",
    "    \"\"\"Representa la transición de un estado al siguiente\"\"\"\n",
    "    prev_state: int              # Estado origen de la transición\n",
    "    next_state: int              # Estado destino de la transición\n",
    "    action: Action               # Acción que provocó esta transición\n",
    "    reward: typing.SupportsFloat # Recompensa obtenida"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estado lo representaremos como un entero ya que, en este problema en concreto, sólo tenemos un estado. Hemos preferido mantener aún así los atributos `prev_state` y `next_state` para que el código se corresponda con las fórmulas explicadas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1. Definición del agente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos el agente que aprenderá a jugar a partir de las jugadas de los demás. Debe implementar los métodos indicados para que cumplan la firma y el comentario.\n",
    "\n",
    "Se permite escribir código sólo entre el espacio delimitado por los comentarios `# START` y `# END`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import requests\n",
    "\n",
    "\n",
    "# Hemos cambiado algún nombre por comodidad\n",
    "class Agent:\n",
    "    def __init__(self, name: str, q_table: typing.Any=None):\n",
    "        \"\"\"Inicializa este objeto.\n",
    "        \n",
    "        :param name: El nombre del agente, para identificarle.\n",
    "        :param q_table: Una tabla q de valores. Es opcional.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        if q_table:\n",
    "            self.q_table = q_table\n",
    "        else:\n",
    "            # Inicializa la Q-table a 0\n",
    "            self.q_table = {b.value: 0.0 for b in Action}\n",
    "            self.updated_table = self.q_table.copy()\n",
    "            \n",
    "\n",
    "    def decide(self, state:int, epsilon: typing.SupportsFloat=0) -> Action:\n",
    "        \"\"\"Decide la acción a ejecutar.\n",
    "        \n",
    "        :param state: El estado en el que nos encontramos.\n",
    "        :param epsilon: Un valor entre 0 y 1 que representa, según la estrategia\n",
    "            ε-greedy, la probabilidad de que la acción sea elegida de manera\n",
    "            aleatoria de entre todas las opciones posibles. Es opcional, y si\n",
    "            no se especifica su valor es 0 (sin probabilidades de que se elija\n",
    "            una acción aleatoria).\n",
    "        :param returns: La acción a ejecutar.\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            # Con probabilidad epsilon, elegir una acción random\n",
    "            return np.random.choice(list(Action))\n",
    "        if state in self.q_table:\n",
    "            # Si el estado está en la Q-table, elige el que tenga el valor más alto\n",
    "            argmax = np.argmax(list(self.q_table[state].values()))\n",
    "            return list(Action)[argmax]\n",
    "        else:\n",
    "            # Si no lo está, elige uno random\n",
    "            self.q_table[state] = {action.value: 0.0 for action in Action}\n",
    "            return np.random.choice(list(Action))\n",
    "        \n",
    "\n",
    "    def update(self, t: Transition, alpha=0.1, gamma=0.95):\n",
    "        \"\"\"Actualiza el estado interno de acuerdo a la experiencia vivida.\n",
    "        \n",
    "        :param transition: Toda la información correspondiente a la transición\n",
    "            de la que queremos aprender.\n",
    "        :param 𝛼: El factor de aprendizaje del cambio en el valor q. Por\n",
    "            defecto es 0.1\n",
    "        :param 𝛾: La influencia de la recompensa a largo plazo en el valor q a\n",
    "            actualizar. Va de 0 (sin influencia) a 1 (misma influencia que el\n",
    "            valor actual). Por defecto es 0.95.\n",
    "        \"\"\"\n",
    "        if t.prev_state not in self.q_table:\n",
    "            # Si el prev estado no está, añadirlo\n",
    "            self.q_table[t.prev_state] = {action.value: 0.0 for action in Action}\n",
    "        current_q = self.q_table[t.prev_state][t.action.value]\n",
    "        next_q = max(list(self.q_table[t.next_state].values()))\n",
    "        td_target = t.reward + gamma * next_q\n",
    "        td_error = td_target - current_q\n",
    "        new_q = current_q + alpha * td_error\n",
    "        self.q_table[t.prev_state][t.action.value] = new_q\n",
    "        self.updated_table = self.q_table.copy()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Representación textual de la tabla Q del agente.\n",
    "        \n",
    "        :returns: Una cadena indicando la estructura interna de la tabla Q.\n",
    "        \"\"\"\n",
    "        table_str = \"Agent - Q Table:\\n\"\n",
    "        for state, actions in self.q_table.items():\n",
    "            table_str += f\"State: {state}\\n\"\n",
    "            if isinstance(actions, float):\n",
    "                table_str += f\"  {Action(state).name}: {actions}\\n\"\n",
    "            else:\n",
    "                for action, value in actions.items():\n",
    "                    table_str += f\"  {action}: {value}\\n\"\n",
    "        return table_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrenamiento usaremos el dataset localizado en <https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn>, el cual contiene una secuencia de opciones que ha tomado un jugador en una partida ficticia de este juego. Comenzamos cargando el fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.trn'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el dataset descargado, ya podemos realizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝜀 = 1\n",
    "𝛿𝜀 = 𝜀 / len(player2_actions)\n",
    "\n",
    "game = Game()\n",
    "agent = Agent(name='Agent')\n",
    "\n",
    "state = 0  # El entorno (juego) no tiene estado, así que siempre será el mismo\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state, 𝛆)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "\n",
    "    # Actualizamos el agente\n",
    "    agent.update(Transition(\n",
    "        prev_state=state,\n",
    "        next_state=state,\n",
    "        action=p1_action,\n",
    "        reward=reward\n",
    "    ))\n",
    "\n",
    "    # Actualizamos 𝜀\n",
    "    𝜀 -= 𝛿𝜀 if 𝜀 > 0 else 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras el entrenamiento, podemos ver cómo están de repartidos los valores de la tabla $Q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué tal se comporta el con un conjunto de datos que nunca ha visto del mismo contrincante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://blazaid.github.io/Aprendizaje-profundo/Datasets/rock-paper-scissors-lizard-spock.tst'\n",
    "\n",
    "player2_actions = []\n",
    "with urllib.request.urlopen(dataset_url) as f:\n",
    "    for line in f:\n",
    "        move = line.decode('utf-8').strip().upper()\n",
    "        if move:\n",
    "            player2_actions.append(Action[move])\n",
    "\n",
    "stats = collections.defaultdict(int)\n",
    "state = 0\n",
    "for p2_action in player2_actions:\n",
    "    p1_action = agent.decide(state)\n",
    "    reward = game.play(p1_action, p2_action)\n",
    "    if reward == 1:\n",
    "        stats['wins'] += 1\n",
    "    elif reward == -1:\n",
    "        stats['loses'] += 1\n",
    "    else:\n",
    "        stats['ties'] += 1\n",
    "\n",
    "print(dict(stats))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo visto ha aprendido un poco del sesgo que tenía el contrincante. ¡Bien por nuestro agente! Ya está un poco más cerca de dominar el mundo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2. Nuevo agente para competición"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer las cosas un poco más interesantes, vamos a hacer una pequeña competición. Se trata de desarrollar un agente como el que acabamos de hacer, pero donde los contrincantes serán el resto de agentes de los grupos. Se permite cualquier implementación, siempre y cuando:\n",
    "\n",
    "- Herede de la clase `Agent` suministrada.\n",
    "- El método `__init__` no admita ningún parámetro adicional.\n",
    "\n",
    "En la siguiente celda se ofrece el esqueleto del agente que competirá. Como se puede ver:\n",
    "\n",
    "- Tiene un nombre\n",
    "- A la hora de decidir la acción recibirá un estado, que siempre será 0 y devolverá la opción a realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Agent(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "        \n",
    "        :param name: El nombre del agente.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        # START\n",
    "        # END\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decide(self, state:int) -> Action:\n",
    "        \"\"\"Decide la acción a llevar a cabo dado el estado actual.\n",
    "        \n",
    "        :param state: El estado en el que se encuentra el agente.\n",
    "        :returns: La acción a llevar a cabo.\n",
    "        \"\"\"\n",
    "        # START\n",
    "        # END\n",
    "    \n",
    "    def update(self, transition: Transition):\n",
    "        \"\"\"Actualiza (si es necesario) el estado interno del agente.\n",
    "        \n",
    "        :param transition: La información de la transición efectuada.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, un par de posibles implementaciones (que no usan nada de $q$-learning, y que no se recomiendan) son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Botnifacio(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Botnifacio')\n",
    "    \n",
    "    def decide(self, state:int) -> Action:\n",
    "        return np.random.choice(list(Action))\n",
    "\n",
    "\n",
    "class Gustabot(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Gustabot')\n",
    "        self.weights = np.random.random(5)\n",
    "        self.weights /= sum(self.weights)\n",
    "    \n",
    "    def decide(self, state:int) -> Action:\n",
    "        return np.random.choice(list(Action), p=self.weights)\n",
    "    \n",
    "    def update(self, transition: Transition):\n",
    "        self.weights = np.random.random(5)\n",
    "        self.weights /= sum(self.weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente se deberá desarrollar en la siguiente celda, la cual incluirá todo lo necesario para instanciarlo y que funcione. Se recomienda la competición entre grupos antes de la entrega final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar aquí al agente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La competición será una liga entre los agentes de cada grupo a 100 partidas.\n",
    "\n",
    "Antes de dichas 100 partidas, se hará un previo de 10000 partidas con el fin de que cada uno de los agentes \"aprenda\" a competir contra el contrario. Sólo en esta fase se invocará el método `update`.\n",
    "\n",
    "La competición se realizará como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "FRIENDLY_SETS = 10000\n",
    "COMPETITION_SETS = 10\n",
    "\n",
    "competitors = [\n",
    "    Gustabot(),\n",
    "    Botnifacio(),\n",
    "]\n",
    "\n",
    "leaderboard = {c: 0 for c in sorted(competitors, key=lambda x: x.__str__())}\n",
    "game = Game()\n",
    "for p1, p2 in itertools.combinations(leaderboard.keys(), 2):\n",
    "    # Amistoso\n",
    "    s = 0\n",
    "    for i in range(FRIENDLY_SETS):\n",
    "        a1 = p1.decide(s)\n",
    "        a2 = p2.decide(s)\n",
    "        reward = game.play(a1, a2)\n",
    "        p1.update(Transition(prev_state=s, next_state=s, action=a1, reward=reward))\n",
    "        p2.update(Transition(prev_state=s, next_state=s, action=a2, reward=-reward))\n",
    "    \n",
    "    # Competición\n",
    "    s = 0\n",
    "    r1 = r2 = 0\n",
    "    for i in range(COMPETITION_SETS):\n",
    "        a1 = p1.decide(s)\n",
    "        a2 = p2.decide(s)\n",
    "        reward = game.play(a1, a2)\n",
    "        r1 += reward\n",
    "        r2 -= reward\n",
    "\n",
    "    # Actualización de marcadores globales\n",
    "    leaderboard[p1] += r1\n",
    "    leaderboard[p2] += r2\n",
    "    \n",
    "    print(f'{p1}: {r1}, {p2}: {r2}')\n",
    "    \n",
    "\n",
    "print('LEADERBOARD')\n",
    "for c, r in sorted(leaderboard.items(), key=lambda t: t[1], reverse=True):\n",
    "    print(f'{r:<10}\\t{c}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La calificación de cada grupo irá en función de la puntuación final que haya conseguido."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumiendo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos desarrollado un agente para un juego diferente que aprender a tomar decisiones óptimas en cada situación, a partir de la retroalimentación proporcionada por el entorno del juego. Hemos observado cómo el agente ha ido mejorando gradualmente su desempeño en el juego, a medida que ha acumulado experiencia y ha ajustado sus valores de Q.\n",
    "\n",
    "Además, hemos podido comprobar cómo el agente es capaz de adaptarse a diferentes estrategias de juego de su oponente, lo que demuestra su capacidad para generalizar y tomar decisiones en situaciones nuevas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
